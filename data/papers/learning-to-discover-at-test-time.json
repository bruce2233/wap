{
  "slug": "learning-to-discover-at-test-time",
  "title": {
    "en": "Learning to Discover at Test Time",
    "zh": "测试时学习以“发现”为目标"
  },
  "subtitle": {
    "en": "Test-Time Training to Discover (TTT-Discover)",
    "zh": "测试时训练以发现（TTT-Discover）"
  },
  "summary": {
    "hs": {
      "en": "An LLM keeps learning during testing to beat the best-known answer for a single problem.",
      "zh": "LLM 在测试阶段继续学习，目标是超过单个问题的最好已知解。"
    },
    "grad": {
      "en": "Test-time RL optimized for a single best solution under continuous rewards, not average performance.",
      "zh": "在连续奖励场景下的测试时强化学习，目标是单一最优解而非平均性能。"
    }
  },
  "authors": [
    "Mert Yuksekgonul",
    "Daniel Koceja",
    "Xinhao Li",
    "Federico Bianchi",
    "Jed McCaleb",
    "Xiaolong Wang",
    "Jan Kautz",
    "Yejin Choi",
    "James Zou",
    "Carlos Guestrin",
    "Yu Sun"
  ],
  "arxivId": "2601.16175",
  "date": "2026-01-22",
  "venue": "arXiv cs.LG / cs.AI",
  "tags": ["Test-Time Training", "Reinforcement Learning", "Discovery", "LLM"],
  "links": {
    "arXiv": "https://arxiv.org/abs/2601.16175",
    "PDF": "https://arxiv.org/pdf/2601.16175",
    "Project": "https://test-time-training.github.io/discover/",
    "Code": "https://github.com/test-time-training/discover"
  },
  "sections": [
    {
      "id": "overview",
      "title": {
        "en": "Overview",
        "zh": "概览"
      },
      "blocks": [
        {
          "level": "hs",
          "type": "text",
          "text": {
            "en": "The paper asks: can we keep training during evaluation so the model discovers a stronger solution for the exact problem in front of it?",
            "zh": "论文提出：能否在评估阶段继续训练，让模型为当前这个问题发现更强解？"
          }
        },
        {
          "level": "grad",
          "type": "list",
          "items": {
            "en": [
              "Test-time reinforcement learning: the model keeps updating using feedback from the test problem.",
              "The objective focuses on the best single solution rather than average reward.",
              "Experiments target continuous-reward domains with verifiable evaluation."
            ],
            "zh": [
              "测试时强化学习：模型在评估阶段利用当前问题的反馈继续更新。",
              "目标函数强调“最好单解”而非平均回报。",
              "实验聚焦可验证、连续奖励的问题。"
            ]
          }
        }
      ]
    },
    {
      "id": "setup",
      "title": {
        "en": "Problem setup",
        "zh": "问题定义"
      },
      "blocks": [
        {
          "level": "hs",
          "type": "list",
          "items": {
            "en": [
              "A task description tells the model what to solve.",
              "A candidate solution can be code or a mathematical construction.",
              "A scoring function measures how good the solution is.",
              "Discovery means beating the current best score."
            ],
            "zh": [
              "任务描述告诉模型要解决什么问题。",
              "候选解可能是代码或数学构造。",
              "评分函数用来衡量解的好坏。",
              "发现 = 超过当前最好分数。"
            ]
          }
        },
        {
          "level": "grad",
          "type": "list",
          "items": {
            "en": [
              "State is a candidate solution; reward is a continuous score from an evaluator.",
              "Discovery is defined as any solution whose reward exceeds the best-known result.",
              "The paper focuses on problems where rewards are available and stable."
            ],
            "zh": [
              "状态是候选解；奖励来自评估器的连续分数。",
              "发现定义为任何超过当前最好结果的解。",
              "论文聚焦奖励可得且稳定的问题。"
            ]
          }
        }
      ]
    },
    {
      "id": "method",
      "title": {
        "en": "Method",
        "zh": "方法"
      },
      "blocks": [
        {
          "level": "hs",
          "type": "text",
          "text": {
            "en": "The model proposes a solution, gets a score, learns from it, and repeats — all during testing.",
            "zh": "模型提出解法、得到分数、立即学习、再继续尝试——全部发生在测试阶段。"
          }
        },
        {
          "level": "grad",
          "type": "list",
          "items": {
            "en": [
              "Test-time training updates the model with experience from the current problem.",
              "The learning objective and search strategy prioritize the most promising solutions.",
              "This shifts compute toward improving a single target solution."
            ],
            "zh": [
              "测试时训练使用当前问题的经验直接更新模型。",
              "学习目标与搜索策略偏向最有潜力的候选解。",
              "计算资源集中在提升单一目标解。"
            ]
          }
        }
      ]
    },
    {
      "id": "results",
      "title": {
        "en": "Results",
        "zh": "结果"
      },
      "blocks": [
        {
          "level": "hs",
          "type": "list",
          "items": {
            "en": [
              "New best results across math, GPU kernels, algorithm contests, and biology.",
              "Up to ~2× faster GPU kernels than prior art.",
              "Scores verified by experts or competition organizers."
            ],
            "zh": [
              "在数学、GPU kernel、算法竞赛、生物任务上取得新最好结果。",
              "GPU kernel 最快可达约 2× 提升。",
              "结果由专家或竞赛组织者确认。"
            ]
          }
        },
        {
          "level": "grad",
          "type": "list",
          "items": {
            "en": [
              "Erdős overlap: 0.380876 (best human 0.380927).",
              "Autocorrelation AC1: 1.50287 (best human 1.50973).",
              "TriMul kernels: A100 2198 μs; H100 1161 μs.",
              "AtCoder score: 567,062 (best human 566,997).",
              "Single-cell denoising: 0.71 (best human 0.64)."
            ],
            "zh": [
              "Erdős 重叠：0.380876（人类最好 0.380927）。",
              "自相关 AC1：1.50287（人类最好 1.50973）。",
              "TriMul kernel：A100 2198 μs；H100 1161 μs。",
              "AtCoder 得分：567,062（人类最好 566,997）。",
              "单细胞去噪：0.71（人类最好 0.64）。"
            ]
          }
        }
      ]
    },
    {
      "id": "repro",
      "title": {
        "en": "Reproducibility & cost",
        "zh": "复现与成本"
      },
      "blocks": [
        {
          "level": "hs",
          "type": "list",
          "items": {
            "en": [
              "Uses an open model (OpenAI gpt-oss-120b).",
              "Public code is available for reproduction.",
              "Reported cost is only a few hundred dollars per problem."
            ],
            "zh": [
              "使用开源模型（OpenAI gpt-oss-120b）。",
              "公开代码可复现结果。",
              "报告成本为每个问题几百美元。"
            ]
          }
        },
        {
          "level": "grad",
          "type": "text",
          "text": {
            "en": "The paper emphasizes open-model reproducibility and notes test-time training runs on Tinker cost only a few hundred dollars per problem.",
            "zh": "论文强调开源模型可复现性，并指出在 Tinker 上的测试时训练成本约为每个问题几百美元。"
          }
        }
      ]
    },
    {
      "id": "limits",
      "title": {
        "en": "Scope & limitations",
        "zh": "范围与局限"
      },
      "blocks": [
        {
          "level": "hs",
          "type": "list",
          "items": {
            "en": [
              "Needs a scoring function to evaluate each attempt.",
              "Best suited for problems with reliable, continuous rewards."
            ],
            "zh": [
              "需要可计算的评分函数。",
              "更适合连续、稳定奖励的问题。"
            ]
          }
        },
        {
          "level": "grad",
          "type": "text",
          "text": {
            "en": "The method is evaluated on continuous-reward tasks; extending to sparse or binary rewards is left open.",
            "zh": "方法当前聚焦连续奖励任务，对稀疏或二值奖励的扩展仍是开放问题。"
          }
        }
      ]
    }
  ]
}
