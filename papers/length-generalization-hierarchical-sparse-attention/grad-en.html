<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Grad English | Length Generalization in Hierarchical Sparse Attention</title>
  <meta name="description" content="Graduate-level summary of Understanding and Improving Length Generalization in Hierarchical Sparse Attention Models." />
  <link rel="preconnect" href="https://fonts.googleapis.com" />
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  <link href="https://fonts.googleapis.com/css2?family=Noto+Sans+SC:wght@300;400;500;600;700&family=Space+Grotesk:wght@400;500;600;700&display=swap" rel="stylesheet" />
  <link rel="stylesheet" href="/papers/length-generalization-hierarchical-sparse-attention/styles.css" />
</head>
<body>
  <div class="page">
    <header class="topbar">
      <div class="brand">
        <span>WAP PAPER</span>
        <h1>Length Generalization in Hierarchical Sparse Attention</h1>
      </div>
      <nav class="nav-links">
        <a href="/">WAP Index</a>
        <a href="/papers/length-generalization-hierarchical-sparse-attention/index.html">All Versions</a>
        <a href="/papers/length-generalization-hierarchical-sparse-attention/hs-en.html">HS EN</a>
        <a href="/papers/length-generalization-hierarchical-sparse-attention/hs-zh.html">高中</a>
        <a href="/papers/length-generalization-hierarchical-sparse-attention/grad-zh.html">研究生</a>
      </nav>
    </header>

    <section class="hero reveal">
      <div class="hero-card">
        <h2>Graduate-level takeaway</h2>
        <p>The paper provides a systematic dissection of hierarchical sparse attention models, with ablations and theory that isolate three design principles enabling training-free length extrapolation. A model trained on 4K context generalizes to 32M tokens on RULER and BABILong.</p>
        <div class="badge-row">
          <span class="badge">Grad · EN</span>
          <span class="badge">Hierarchical Sparse Attention</span>
          <span class="badge">Length Extrapolation</span>
        </div>
      </div>
      <div class="hero-card">
        <h2>Paper facts</h2>
        <div class="fact-grid">
          <div class="fact-item">Authors: Jiaqi Leng, Xiang Hu, Junxiong Wang, Jianguo Li, Wei Wu, Yucheng Lu</div>
          <div class="fact-item">Date: 20 Oct 2025</div>
          <div class="fact-item">Venue: arXiv (cs.CL) preprint</div>
          <div class="fact-item">arXiv: 2510.17196</div>
          <div class="fact-item">DOI: 10.48550/arXiv.2510.17196</div>
        </div>
      </div>
    </section>

    <section id="problem" class="section reveal">
      <h3>Problem setup / 背景</h3>
      <p>Standard attention has quadratic cost and degrades on very long inputs. Sparse or linear-time alternatives can scale but often lose long-range fidelity. Chunk-based hierarchical sparse attention is promising for long-context scaling, yet it is unclear which architectural components actually enable length generalization versus short-context performance.</p>
    </section>

    <section id="method" class="section reveal">
      <h3>Method / 方法</h3>
      <p>The authors build a unified framework to ablate components of hierarchical sparse attention and provide theoretical motivation for intra-chunk processing and landmark generation. The key principles are:</p>
      <ul>
        <li><strong>Expressive chunk encoder + CLS token:</strong> Intra-chunk non-linear processing and a dedicated summary token are required for effective retrieval of relevant chunks.</li>
        <li><strong>Bypassing Residual Path (BRP):</strong> Global information should be injected back into token representations so that retrieval signals directly influence token-level computation.</li>
        <li><strong>Enforced selection sparsity:</strong> Sparse selection during pre-training reduces train/test mismatch and improves extrapolation to longer contexts.</li>
      </ul>
    </section>

    <section id="experiments" class="section reveal">
      <h3>Experiments & Results / 实验与结果</h3>
      <p>The paper reports comprehensive ablations that confirm each principle is necessary for length extrapolation. The final design yields state-of-the-art training-free length generalization on long-context benchmarks.</p>
      <div class="metric-grid">
        <div class="metric-card">
          <h4>4K → 32M</h4>
          <span>Trained on 4K tokens; tested up to 32,000,000 tokens.</span>
        </div>
        <div class="metric-card">
          <h4>RULER + BABILong</h4>
          <span>Benchmarks for long-context reasoning and memory.</span>
        </div>
        <div class="metric-card">
          <h4>SOTA</h4>
          <span>New state-of-the-art in training-free length extrapolation.</span>
        </div>
        <div class="metric-card">
          <h4>Ablations</h4>
          <span>Removing any core principle harms extrapolation.</span>
        </div>
      </div>
    </section>

    <section id="limitations" class="section reveal">
      <h3>Limitations / 局限</h3>
      <p>The abstract does not enumerate limitations. Likely constraints include reliance on specific benchmark suites, absence of public code on arXiv, and unknown trade-offs in latency/memory on real-world deployments. Further validation on other domains (e.g., long-form retrieval, dialogue, multi-document QA) would strengthen conclusions.</p>
    </section>

    <section id="resources" class="section reveal">
      <h3>Resources / 资源</h3>
      <div class="resource-list">
        <div class="resource">
          <span>Paper (arXiv abstract)</span>
          <a href="https://arxiv.org/abs/2510.17196" target="_blank" rel="noreferrer">Open</a>
        </div>
        <div class="resource">
          <span>PDF</span>
          <a href="https://arxiv.org/pdf/2510.17196.pdf" target="_blank" rel="noreferrer">Download</a>
        </div>
        <div class="resource">
          <span>DOI</span>
          <a href="https://doi.org/10.48550/arXiv.2510.17196" target="_blank" rel="noreferrer">10.48550/arXiv.2510.17196</a>
        </div>
      </div>
    </section>

    <section id="citation" class="section reveal">
      <h3>Suggested citation / 推荐引用</h3>
      <div class="citation">
        Leng, Jiaqi, Xiang Hu, Junxiong Wang, Jianguo Li, Wei Wu, and Yucheng Lu. 2025. “Understanding and Improving Length Generalization in Hierarchical Sparse Attention Models.” arXiv:2510.17196. https://doi.org/10.48550/arXiv.2510.17196.
      </div>
      <button class="copy-btn" type="button" data-copy="Leng, Jiaqi, Xiang Hu, Junxiong Wang, Jianguo Li, Wei Wu, and Yucheng Lu. 2025. Understanding and Improving Length Generalization in Hierarchical Sparse Attention Models. arXiv:2510.17196. https://doi.org/10.48550/arXiv.2510.17196.">Copy citation</button>
    </section>

    <footer class="footer">This is the Grad English version.</footer>
  </div>

  <script src="/papers/length-generalization-hierarchical-sparse-attention/script.js"></script>
</body>
</html>
