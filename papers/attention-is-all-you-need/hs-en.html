<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Attention Is All You Need | HS EN</title>
  <meta name="description" content="High-school friendly explanation of Attention Is All You Need and the Transformer." />
  <link rel="preconnect" href="https://fonts.googleapis.com" />
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  <link href="https://fonts.googleapis.com/css2?family=IBM+Plex+Mono:wght@400;600&family=Noto+Sans+SC:wght@300;400;500;600;700&family=Space+Grotesk:wght@400;500;600;700&display=swap" rel="stylesheet" />
  <link rel="stylesheet" href="/papers/attention-is-all-you-need/styles.css" />
</head>
<body>
  <div class="backdrop" aria-hidden="true">
    <div class="flare one"></div>
    <div class="flare two"></div>
    <div class="flare three"></div>
  </div>

  <div class="page">
    <header class="hero reveal">
      <div>
        <div class="eyebrow">WAP PAPER / HS / EN</div>
        <h1>Attention Is All You Need</h1>
        <p class="subtitle">A clear, high-school level walkthrough of the Transformer paper, following the same story arc as the original: motivation -> architecture -> training -> results.</p>
        <div class="hero-actions">
          <a class="btn primary" href="/papers/attention-is-all-you-need/index.html">All Versions</a>
          <a class="btn" href="/papers/attention-is-all-you-need/grad-en.html">Grad EN</a>
          <a class="btn" href="/papers/attention-is-all-you-need/hs-zh.html">高中 中文</a>
          <a class="btn" href="/papers/attention-is-all-you-need/grad-zh.html">研究生 中文</a>
        </div>
      </div>
      <div class="hero-panel">
        <div class="panel-title">Paper Facts</div>
        <div class="attention-grid" aria-hidden="true"></div>
        <div class="meta-grid">
          <div class="meta-item">
            <span class="meta-label">Authors</span>
            <div class="meta-value">Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, Polosukhin</div>
          </div>
          <div class="meta-item">
            <span class="meta-label">Venue</span>
            <div class="meta-value">NeurIPS (NIPS) 2017 / arXiv 1706.03762</div>
          </div>
          <div class="meta-item">
            <span class="meta-label">Problem</span>
            <div class="meta-value">Machine translation and sequence-to-sequence modeling.</div>
          </div>
          <div class="meta-item">
            <span class="meta-label">Core Claim</span>
            <div class="meta-value">Pure attention can outperform RNN/CNN translation while training faster.</div>
          </div>
        </div>
      </div>
    </header>

    <nav class="section-nav reveal" aria-label="Page sections">
      <a href="#abstract">Abstract</a>
      <a href="#intro">Intro</a>
      <a href="#architecture">Architecture</a>
      <a href="#why">Why Self-Attention</a>
      <a href="#training">Training</a>
      <a href="#results">Results</a>
      <a href="#future">Outlook</a>
      <a href="#resources">Resources</a>
      <a href="#citation">Citation</a>
    </nav>

    <section id="abstract" class="chapter reveal" data-section>
      <h2>Abstract (plain version)</h2>
      <p>The paper introduces the Transformer: a sequence-to-sequence model that uses attention only. It removes recurrence and convolution, so training can run in parallel and finish faster, while translation quality improves.</p>
      <div class="callout">Big result: strong BLEU scores on WMT14 English-German and English-French with much shorter training time.</div>
    </section>

    <section id="intro" class="chapter reveal" data-section>
      <h2>Introduction & background</h2>
      <p>Machine translation turns one sentence into another. Earlier systems (RNNs and CNNs) process tokens in order, which makes training slow and makes it harder to connect distant words.</p>
      <p>Attention was already helpful, but it usually sat on top of a recurrent network. This paper asks: what if attention is the only building block?</p>
      <div class="highlight-row">
        <div class="highlight-card">
          <strong>Goal</strong>
          <div>Remove sequential computation so training can parallelize.</div>
        </div>
        <div class="highlight-card">
          <strong>Idea</strong>
          <div>Let every token directly look at every other token using self-attention.</div>
        </div>
        <div class="highlight-card">
          <strong>Outcome</strong>
          <div>Faster training + better translation quality.</div>
        </div>
      </div>
    </section>

    <section id="architecture" class="chapter reveal" data-section>
      <h2>Transformer architecture</h2>
      <p>The model keeps the classic encoder-decoder layout but swaps the guts. Both encoder and decoder are stacks of identical layers (six layers each in the base model).</p>
      <div class="highlight-row">
        <div class="highlight-card">
          <strong>Encoder layer</strong>
          <div>Self-attention + a small feed-forward network.</div>
        </div>
        <div class="highlight-card">
          <strong>Decoder layer</strong>
          <div>Masked self-attention + encoder-decoder attention + feed-forward.</div>
        </div>
        <div class="highlight-card">
          <strong>Stability</strong>
          <div>Residual connections + layer normalization around every block.</div>
        </div>
      </div>
      <h3>Scaled dot-product attention</h3>
      <p>Each token creates three vectors: Query (Q), Key (K), and Value (V). The model compares Q with all K's, turns those scores into weights, and uses them to mix the V's.</p>
      <div class="callout">Scaling by sqrt(d_k) keeps scores in a stable range so softmax doesn&#39;t saturate.</div>
      <h3>Multi-head attention</h3>
      <p>Instead of a single attention map, the Transformer uses multiple heads (8 in the base model). Each head can focus on a different pattern, such as syntax or long-range meaning.</p>
      <h3>Position-wise feed-forward</h3>
      <p>After attention, each position goes through the same two-layer MLP (a small neural network). This adds non-linearity and extra capacity.</p>
      <h3>Embeddings & positional encoding</h3>
      <p>Because there is no recurrence, the model adds positional encodings to word embeddings. The paper uses sine/cosine waves to represent position, helping the model know word order.</p>
    </section>

    <section id="why" class="chapter reveal" data-section>
      <h2>Why self-attention?</h2>
      <p>Self-attention connects every token to every other token in one hop, so long-distance relationships are easy to learn.</p>
      <ul>
        <li>Parallel: all tokens are processed together.</li>
        <li>Short paths: any two words are one attention step apart.</li>
        <li>Tradeoff: attention cost grows with sequence length (quadratic in length).</li>
      </ul>
    </section>

    <section id="training" class="chapter reveal" data-section>
      <h2>Training setup</h2>
      <p>The authors train on WMT14 English-German (4.5M sentence pairs) and WMT14 English-French (36M pairs). Sentences are split into subword tokens using byte-pair encoding.</p>
      <div class="highlight-row">
        <div class="highlight-card">
          <strong>Batching</strong>
          <div>About 25k source + 25k target tokens per batch.</div>
        </div>
        <div class="highlight-card">
          <strong>Hardware</strong>
          <div>8x NVIDIA P100 GPUs.</div>
        </div>
        <div class="highlight-card">
          <strong>Speed</strong>
          <div>Base model: 100k steps about 12 hours. Big model: 300k steps about 3.5 days.</div>
        </div>
      </div>
      <p>Optimization uses Adam with a warmup schedule (first 4k steps) plus dropout and label smoothing (both 0.1) for regularization.</p>
    </section>

    <section id="results" class="chapter reveal" data-section>
      <h2>Results</h2>
      <p>The Transformer (big) reaches 28.4 BLEU on English-German and 41.8 BLEU on English-French, beating prior systems while training faster.</p>
      <div class="data-grid">
        <div class="data-chip">EN-DE: 28.4 BLEU</div>
        <div class="data-chip">EN-FR: 41.8 BLEU</div>
        <div class="data-chip">Training: 3.5 days on 8 GPUs</div>
      </div>
    </section>

    <section id="future" class="chapter reveal" data-section>
      <h2>Generalization & outlook</h2>
      <p>The paper also applies the Transformer to English constituency parsing and shows it generalizes beyond translation.</p>
      <p>Limitation: self-attention is O(n^2) in sequence length. The authors suggest exploring restricted attention for very long sequences and applying the model to other modalities.</p>
    </section>

    <section id="resources" class="chapter reveal" data-section>
      <h2>Resources</h2>
      <div class="resource-list">
        <div class="resource-item">
          <span>arXiv Abstract</span>
          <a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noreferrer">1706.03762</a>
        </div>
        <div class="resource-item">
          <span>Paper PDF</span>
          <a href="https://arxiv.org/pdf/1706.03762.pdf" target="_blank" rel="noreferrer">Download PDF</a>
        </div>
        <div class="resource-item">
          <span>NeurIPS Proceedings</span>
          <a href="https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html" target="_blank" rel="noreferrer">NIPS 2017</a>
        </div>
        <div class="resource-item">
          <span>Reference Code</span>
          <a href="https://github.com/tensorflow/tensor2tensor" target="_blank" rel="noreferrer">Tensor2Tensor</a>
        </div>
      </div>
    </section>

    <section id="citation" class="chapter reveal" data-section>
      <h2>Citation</h2>
      <pre id="citation-text" class="citation">@inproceedings{vaswani2017attention,
  title={Attention Is All You Need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  booktitle={Advances in Neural Information Processing Systems},
  year={2017}
}</pre>
      <button class="copy-btn" data-copy-target="citation-text">Copy BibTeX</button>
    </section>

    <footer class="footer">WAP / Attention Is All You Need (HS EN)</footer>
  </div>

  <script src="/papers/attention-is-all-you-need/script.js"></script>
</body>
</html>
