<!doctype html>
<html lang="zh-Hans">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Attention Is All You Need | 研究生 中文</title>
  <meta name="description" content="研究生版本：包含公式与训练细节的 Transformer 论文解读。" />
  <link rel="preconnect" href="https://fonts.googleapis.com" />
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  <link href="https://fonts.googleapis.com/css2?family=IBM+Plex+Mono:wght@400;600&family=Noto+Sans+SC:wght@300;400;500;600;700&family=Space+Grotesk:wght@400;500;600;700&display=swap" rel="stylesheet" />
  <link rel="stylesheet" href="/papers/attention-is-all-you-need/styles.css" />
</head>
<body>
  <div class="backdrop" aria-hidden="true">
    <div class="flare one"></div>
    <div class="flare two"></div>
    <div class="flare three"></div>
  </div>

  <div class="page">
    <header class="hero reveal">
      <div>
        <div class="eyebrow">WAP PAPER · 研究生 · 中文</div>
        <h1>Attention Is All You Need</h1>
        <p class="subtitle">研究生版本，按论文原有结构给出关键公式、超参数与实验细节。</p>
        <div class="hero-actions">
          <a class="btn primary" href="/papers/attention-is-all-you-need/index.html">全部版本</a>
          <a class="btn" href="/papers/attention-is-all-you-need/hs-en.html">HS EN</a>
          <a class="btn" href="/papers/attention-is-all-you-need/grad-en.html">Grad EN</a>
          <a class="btn" href="/papers/attention-is-all-you-need/hs-zh.html">高中 中文</a>
        </div>
      </div>
      <div class="hero-panel">
        <div class="panel-title">论文信息</div>
        <div class="attention-grid" aria-hidden="true"></div>
        <div class="meta-grid">
          <div class="meta-item">
            <span class="meta-label">作者</span>
            <div class="meta-value">Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin</div>
          </div>
          <div class="meta-item">
            <span class="meta-label">会议</span>
            <div class="meta-value">NeurIPS (NIPS) 2017 · arXiv 1706.03762</div>
          </div>
          <div class="meta-item">
            <span class="meta-label">模型</span>
            <div class="meta-value">Transformer：多头自注意力的编码器-解码器。</div>
          </div>
          <div class="meta-item">
            <span class="meta-label">论点</span>
            <div class="meta-value">仅用注意力即可达到更强翻译质量，并显著提升并行性。</div>
          </div>
        </div>
      </div>
    </header>

    <nav class="section-nav reveal" aria-label="Page sections">
      <a href="#abstract">摘要</a>
      <a href="#intro">引言</a>
      <a href="#architecture">架构</a>
      <a href="#why">自注意力优势</a>
      <a href="#training">训练</a>
      <a href="#results">结果</a>
      <a href="#future">结论</a>
      <a href="#resources">资源</a>
      <a href="#citation">引用</a>
    </nav>

    <section id="abstract" class="chapter reveal" data-section>
      <h2>摘要（技术版）</h2>
      <p>Transformer 去掉循环与卷积，只用注意力进行序列建模，从而在保持或提升翻译质量的同时获得更高的训练并行度。</p>
    </section>

    <section id="intro" class="chapter reveal" data-section>
      <h2>引言与背景</h2>
      <p>传统 RNN/CNN 编码器-解码器需要顺序计算，长距离依赖路径较长。论文提出用自注意力替代这些结构，并通过多头机制覆盖不同关系模式。</p>
    </section>

    <section id="architecture" class="chapter reveal" data-section>
      <h2>模型架构</h2>
      <p>Transformer 层由两部分组成：多头自注意力 + 位置前馈网络，每个子层外面都有残差连接和层归一化。</p>
      <div class="formula">LayerNorm(x + Sublayer(x))</div>
      <div class="highlight-row">
        <div class="highlight-card">
          <strong>层数</strong>
          <div>N = 6（编码器）+ 6（解码器）。</div>
        </div>
        <div class="highlight-card">
          <strong>维度</strong>
          <div>d_model = 512，d_ff = 2048。</div>
        </div>
        <div class="highlight-card">
          <strong>注意力头</strong>
          <div>h = 8，d_k = d_v = 64。</div>
        </div>
      </div>

      <h3>缩放点积注意力</h3>
      <div class="formula">Attention(Q, K, V) = softmax(QK^T / √d_k) V</div>
      <p>解码器自注意力使用遮罩以保证自回归生成。</p>

      <h3>多头注意力</h3>
      <p>多个头并行计算注意力分布并拼接投影，以捕捉不同关系子空间。</p>

      <h3>位置前馈网络</h3>
      <p>对每个位置独立使用两层全连接 + ReLU：Linear(d_model → d_ff) → ReLU → Linear(d_ff → d_model)。</p>

      <h3>词向量与位置编码</h3>
      <p>使用正弦/余弦位置编码与词向量相加，使模型在无循环结构下仍能表达顺序。</p>
    </section>

    <section id="why" class="chapter reveal" data-section>
      <h2>自注意力的优势</h2>
      <p>自注意力让任意两个位置的路径长度为 1，且可完全并行计算，显著提升训练效率。</p>
      <div class="callout">代价是时间与内存复杂度 O(n²)。论文讨论了通过限制注意力范围来处理超长序列的可能性。</div>
    </section>

    <section id="training" class="chapter reveal" data-section>
      <h2>训练设置</h2>
      <p>WMT14 英德 450 万句对、英法 3600 万句对；BPE 词表约 37k/32k。每个 batch 约 25k 源 + 25k 目标 token。</p>
      <div class="highlight-row">
        <div class="highlight-card">
          <strong>硬件</strong>
          <div>8× NVIDIA P100 GPU。</div>
        </div>
        <div class="highlight-card">
          <strong>训练时长</strong>
          <div>基础模型 100k 步约 12 小时，大模型 300k 步约 3.5 天。</div>
        </div>
        <div class="highlight-card">
          <strong>优化器</strong>
          <div>Adam：β1=0.9，β2=0.98，ε=1e-9。</div>
        </div>
      </div>
      <p>学习率在前 4k 步线性 warmup，之后按 step^-0.5 衰减；dropout=0.1，label smoothing=0.1。</p>
    </section>

    <section id="results" class="chapter reveal" data-section>
      <h2>实验结果</h2>
      <p>Transformer (big) 在 WMT14 EN→DE 达到 28.4 BLEU，在 EN→FR 达到 41.8 BLEU，并能在几天内完成训练。</p>
      <div class="data-grid">
        <div class="data-chip">EN-DE：28.4 BLEU</div>
        <div class="data-chip">EN-FR：41.8 BLEU</div>
        <div class="data-chip">3.5 天（8 卡）</div>
      </div>
      <p>论文还展示 Transformer 在英文成分句法分析上的可行性。</p>
    </section>

    <section id="future" class="chapter reveal" data-section>
      <h2>结论与展望</h2>
      <p>Transformer 证明注意力即可完成高质量序列建模。未来工作包括更高效的注意力、处理超长序列以及拓展到其他模态。</p>
    </section>

    <section id="resources" class="chapter reveal" data-section>
      <h2>资源</h2>
      <div class="resource-list">
        <div class="resource-item">
          <span>arXiv 摘要</span>
          <a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noreferrer">1706.03762</a>
        </div>
        <div class="resource-item">
          <span>论文 PDF</span>
          <a href="https://arxiv.org/pdf/1706.03762.pdf" target="_blank" rel="noreferrer">下载 PDF</a>
        </div>
        <div class="resource-item">
          <span>NeurIPS 论文页</span>
          <a href="https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html" target="_blank" rel="noreferrer">NIPS 2017</a>
        </div>
        <div class="resource-item">
          <span>参考代码</span>
          <a href="https://github.com/tensorflow/tensor2tensor" target="_blank" rel="noreferrer">Tensor2Tensor</a>
        </div>
      </div>
    </section>

    <section id="citation" class="chapter reveal" data-section>
      <h2>引用</h2>
      <pre id="citation-text" class="citation">@inproceedings{vaswani2017attention,
  title={Attention Is All You Need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  booktitle={Advances in Neural Information Processing Systems},
  year={2017}
}</pre>
      <button class="copy-btn" data-copy-target="citation-text">复制 BibTeX</button>
    </section>

    <footer class="footer">WAP · Attention Is All You Need（研究生中文）</footer>
  </div>

  <script src="/papers/attention-is-all-you-need/script.js"></script>
</body>
</html>
