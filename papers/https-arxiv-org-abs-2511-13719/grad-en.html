<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Doc-Researcher (Grad-EN) | WAP</title>
  <link rel="stylesheet" href="/papers/https-arxiv-org-abs-2511-13719/styles.css" />
  <link href="https://fonts.googleapis.com/css2?family=Space+Grotesk:wght@400;500;600;700&display=swap" rel="stylesheet" />
</head>
<body>
  <div class="backdrop" aria-hidden="true"></div>
  <div class="page">
    <header class="hero reveal">
      <div class="eyebrow">GRADUATE EDITION / ENGLISH</div>
      <h1>Doc-Researcher: Overcoming the Multimodal Processing Bottleneck</h1>
      <p class="subtitle">A technical deep-dive into deep multimodal parsing, adaptive retrieval, and agentic evidence synthesis.</p>
    </header>

    <nav class="section-nav reveal">
      <a href="#motivation">Motivation</a>
      <a href="#parsing">Deep Parsing</a>
      <a href="#retrieval">Retrieval Architecture</a>
      <a href="#agents">Agent Workflows</a>
      <a href="#bench">M4DocBench</a>
      <a href="#results">Results</a>
    </nav>

    <section id="motivation" class="chapter reveal" data-section>
      <h2>Motivation & Problem Statement</h2>
      <p>Current "Deep Research" systems (based on LLMs) are largely restricted to text-based web scraping. In professional and scientific domains, knowledge is dense in <strong>highly structured multimodal documents</strong> (PDFs/Scans). Standard RAG (Retrieval-Augmented Generation) pipelines fail here because they often "flatten" the structure, losing vital visual semantics like the relationship between a chart's axes or the hierarchical context of a table.</p>
    </section>

    <section id="parsing" class="chapter reveal" data-section>
      <h2>I. Deep Multimodal Parsing</h2>
      <p>Doc-Researcher employs a parsing engine that preserves <strong>multimodal integrity</strong>. It creates multi-granular representations:</p>
      <ul>
        <li><strong>Chunk-level:</strong> Captures local context including equations and inline symbols.</li>
        <li><strong>Block-level:</strong> Respects logical visual boundaries (e.g., a specific figure with its caption).</li>
        <li><strong>Document-level:</strong> Maintains layout hierarchy and global semantics.</li>
      </ul>
      <div class="callout">Key Innovation: The system maps visual elements to text descriptions while keeping the original pixel features for vision-centric retrieval.</div>
    </section>

    <section id="retrieval" class="chapter reveal" data-section>
      <h2>II. Systematic Hybrid Retrieval</h2>
      <p>The system utilizes an architecture that supports three paradigms:</p>
      <ol>
        <li><strong>Text-only:</strong> Standard semantic search on text chunks.</li>
        <li><strong>Vision-only:</strong> Directly retrieving document segments based on visual similarity.</li>
        <li><strong>Hybrid:</strong> Combining text and vision signals with <em>dynamic granularity selection</em>â€”choosing between fine-grained chunks or broader document context based on query ambiguity.</li>
      </ol>
    </section>

    <section id="agents" class="chapter reveal" data-section>
      <h2>III. Iterative Multi-Agent Workflows</h2>
      <p>Unlike single-pass retrieval, Doc-Researcher uses an agentic loop:</p>
      <ul>
        <li><strong>Planner:</strong> Decomposes complex, multi-hop queries into sub-tasks.</li>
        <li><strong>Searcher:</strong> Executes the hybrid retrieval to find candidates.</li>
        <li><strong>Refiner:</strong> Evaluates retrieved evidence and decides if more searching is needed (iterative accumulation).</li>
        <li><strong>Synthesizer:</strong> Integrates multimodal evidence to form a final, cited answer.</li>
      </ul>
    </section>

    <section id="bench" class="chapter reveal" data-section>
      <h2>M4DocBench & Evaluation</h2>
      <p>To evaluate these capabilities, the authors introduced <strong>M4DocBench</strong> (Multi-modal, Multi-hop, Multi-document, and Multi-turn). It consists of 158 expert-level questions spanning 304 documents. This benchmark requires the model to "connect the dots" across multiple files and modalities.</p>
    </section>

    <section id="results" class="chapter reveal" data-section>
      <h2>Experimental Outcomes</h2>
      <div class="highlight-row">
        <div class="highlight-card">
          <strong>Direct Comparison</strong>
          <div>50.6% accuracy vs. ~15% for state-of-the-art baselines (3.4x improvement).</div>
        </div>
        <div class="highlight-card">
          <strong>Ablation</strong>
          <div>Removing the "Visual Semantics" component caused the largest performance drop, proving layout matters.</div>
        </div>
      </div>
    </section>

    <footer class="footer">WAP - Academic rigor for deep documents.</footer>
  </div>
  <script src="/papers/https-arxiv-org-abs-2511-13719/script.js"></script>
</body>
</html>
