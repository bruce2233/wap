<!doctype html>
<html lang="zh-Hans">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>多模态基础模型的空间智能扩展 | 研究生中文</title>
  <meta name="description" content="研究生级别的 SenseNova-SI 论文解读，包含数据扩展、实验设置与结果分析。" />
  <link rel="preconnect" href="https://fonts.googleapis.com" />
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  <link href="https://fonts.googleapis.com/css2?family=IBM+Plex+Mono:wght@400;600&family=Noto+Sans+SC:wght@300;400;500;600;700&family=Space+Grotesk:wght@400;500;600;700&display=swap" rel="stylesheet" />
  <link rel="stylesheet" href="/papers/https-arxiv-org-abs-2511-13719/styles.css" />
</head>
<body>
  <div class="page">
    <header class="hero reveal">
      <div>
        <div class="eyebrow">WAP 论文 / 研究生 / 中文</div>
        <h1>多模态基础模型的空间智能扩展</h1>
        <p class="subtitle">研究生版本，严格跟随论文结构：空间动机 → 数据扩展与训练 → 基准评测 → 结果与分析。</p>
        <div class="hero-actions">
          <a class="btn primary" href="/papers/https-arxiv-org-abs-2511-13719/index.html">全部版本</a>
          <a class="btn" href="/papers/https-arxiv-org-abs-2511-13719/grad-en.html">Grad EN</a>
          <a class="btn" href="/papers/https-arxiv-org-abs-2511-13719/hs-zh.html">高中 中文</a>
          <a class="btn" href="/papers/https-arxiv-org-abs-2511-13719/hs-en.html">HS EN</a>
        </div>
      </div>
      <div class="hero-panel">
        <div class="panel-title">论文信息</div>
        <div class="meta-grid">
          <div class="meta-item">
            <span class="meta-label">作者</span>
            <div class="meta-value">Zhongang Cai, Ruisi Wang, Chenyang Gu, Fanyi Pu, Junxiang Xu, Yubo Wang, Wanqi Yin, Zhitao Yang, Chen Wei, Qingping Sun, Tongxi Zhou, Jiaqi Li, Hui En Pang, Oscar Qian, Yukun Wei, Zhiqian Lin, Xuanke Shi, Kewang Deng, Xiaoyang Han, Zukai Chen, Xiangyu Fan, Hanming Deng, Lewei Lu, Liang Pan, Bo Li, Ziwei Liu, Quan Wang, Dahua Lin, Lei Yang</div>
          </div>
          <div class="meta-item">
            <span class="meta-label">日期</span>
            <div class="meta-value">初稿 2025-11-17，v3 修订 2025-12-30。</div>
          </div>
          <div class="meta-item">
            <span class="meta-label">研究重点</span>
            <div class="meta-value">空间推理能力、数据规模规律、多模态基座模型。</div>
          </div>
          <div class="meta-item">
            <span class="meta-label">产出</span>
            <div class="meta-value">SenseNova-SI-8M 数据集 + SenseNova-SI 模型系列 + 开源代码。</div>
          </div>
        </div>
      </div>
    </header>

    <nav class="section-nav reveal" aria-label="页面目录">
      <a href="#abstract">摘要</a>
      <a href="#intro">引言</a>
      <a href="#data">数据构建</a>
      <a href="#training">训练策略</a>
      <a href="#benchmarks">评测</a>
      <a href="#results">结果</a>
      <a href="#analysis">分析</a>
      <a href="#limitations">局限</a>
      <a href="#resources">资源</a>
      <a href="#citation">引用</a>
    </nav>

    <section id="abstract" class="chapter reveal" data-section>
      <h2>摘要（技术版）</h2>
      <p>论文指出当前多模态基础模型在空间推理方面存在系统性短板。作者通过扩展空间数据规模（SenseNova-SI-8M），并保持模型架构不变，显著提升空间相关任务表现，同时维持通用多模态能力。</p>
      <div class="callout">核心路线：数据规模驱动的空间智能提升，而非架构创新。</div>
    </section>

    <section id="intro" class="chapter reveal" data-section>
      <h2>引言与问题定义</h2>
      <p>空间智能涵盖位置关系、三维结构、视角变化等能力。论文认为瓶颈来自数据稀缺：主流视觉语言模型训练数据偏重描述性任务，缺乏空间监督。</p>
      <div class="highlight-row">
        <div class="highlight-card">
          <strong>假设</strong>
          <div>大规模空间监督数据能够显著提升空间推理。</div>
        </div>
        <div class="highlight-card">
          <strong>贡献</strong>
          <div>提出 SenseNova-SI 数据集与系统性 scaling 评测。</div>
        </div>
        <div class="highlight-card">
          <strong>结果</strong>
          <div>多项空间基准取得新 SOTA，同时保持通用能力。</div>
        </div>
      </div>
    </section>

    <section id="data" class="chapter reveal" data-section>
      <h2>数据构建与规模扩展</h2>
      <p>SenseNova-SI-8M 汇总 800 万条空间任务样本，涵盖三维结构理解、视角迁移、相对位置、布局推理等，并按能力维度组织，便于进行课程式训练。</p>
      <div class="highlight-row">
        <div class="highlight-card">
          <strong>全量数据</strong>
          <div>SenseNova-SI-8M。</div>
        </div>
        <div class="highlight-card">
          <strong>公开子集</strong>
          <div>SenseNova-SI-800K。</div>
        </div>
        <div class="highlight-card">
          <strong>任务覆盖</strong>
          <div>3D 理解、视角变换、空间布局。</div>
        </div>
      </div>
    </section>

    <section id="training" class="chapter reveal" data-section>
      <h2>训练策略与模型家族</h2>
      <p>作者在 Qwen3-VL、InternVL3、BAGEL 等基座模型上继续训练，并逐步增加空间数据规模，研究性能随数据规模的增长趋势。</p>
      <div class="highlight-row">
        <div class="highlight-card">
          <strong>基座模型</strong>
          <div>Qwen3-VL、InternVL3、BAGEL。</div>
        </div>
        <div class="highlight-card">
          <strong>Scaling 规律</strong>
          <div>空间性能随数据规模近线性增长。</div>
        </div>
        <div class="highlight-card">
          <strong>推理方式</strong>
          <div>引入空间 Chain-of-Thought 展示中间推理。</div>
        </div>
      </div>
    </section>

    <section id="benchmarks" class="chapter reveal" data-section>
      <h2>评测基准</h2>
      <ul>
        <li>VSI-Bench + MMSI：现实图像的空间推理问题。</li>
        <li>MindCube、ViewSpatial、SITE：三维视角与几何推理。</li>
        <li>MMBench-En：通用多模态能力基准。</li>
      </ul>
    </section>

    <section id="results" class="chapter reveal" data-section>
      <h2>定量结果</h2>
      <p>SenseNova-SI 在空间基准全面提升，并维持较高的通用多模态表现：</p>
      <div class="data-grid">
        <div class="data-chip">VSI-Bench: 68.7%</div>
        <div class="data-chip">MMSI: 43.3%</div>
        <div class="data-chip">MindCube: 85.6%</div>
        <div class="data-chip">ViewSpatial: 54.6%</div>
        <div class="data-chip">SITE: 50.1%</div>
        <div class="data-chip">MMBench-En: 84.9%</div>
      </div>
      <table class="table">
        <thead>
          <tr>
            <th>基准</th>
            <th>SenseNova-SI</th>
            <th>对比</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>VSI-Bench</td>
            <td>68.7</td>
            <td>现有模型低 60s</td>
          </tr>
          <tr>
            <td>MindCube</td>
            <td>85.6</td>
            <td>过往最佳低 80s</td>
          </tr>
          <tr>
            <td>ViewSpatial</td>
            <td>54.6</td>
            <td>过往最佳高 40s</td>
          </tr>
          <tr>
            <td>MMBench-En</td>
            <td>84.9</td>
            <td>通用 VLM 基准保持领先</td>
          </tr>
        </tbody>
      </table>
    </section>

    <section id="analysis" class="chapter reveal" data-section>
      <h2>分析与讨论</h2>
      <p>论文分析了数据规模对性能的影响，并指出模型仍可能出现“投机策略”（如依赖表面线索）。作者展示空间推理的中间步骤，说明模型在三维场景中更擅长规划。</p>
      <div class="callout">结论：数据扩展极其有效，但仍需更严格的偏差控制与评估。</div>
    </section>

    <section id="limitations" class="chapter reveal" data-section>
      <h2>局限与未来方向</h2>
      <p>当前模型在高精度几何与复杂遮挡环境仍不稳定，训练成本高。未来可通过更高质量合成数据、结合 3D 监督以及引入 embodied 评测来进一步提升。</p>
    </section>

    <section id="resources" class="chapter reveal" data-section>
      <h2>资源</h2>
      <div class="resource-list">
        <div class="resource-item">
          <span>arXiv 摘要</span>
          <a href="https://arxiv.org/abs/2511.13719" target="_blank" rel="noreferrer">2511.13719</a>
        </div>
        <div class="resource-item">
          <span>论文 PDF</span>
          <a href="https://arxiv.org/pdf/2511.13719.pdf" target="_blank" rel="noreferrer">下载 PDF</a>
        </div>
        <div class="resource-item">
          <span>代码</span>
          <a href="https://github.com/OpenSenseNova/SenseNova-SI" target="_blank" rel="noreferrer">OpenSenseNova/SenseNova-SI</a>
        </div>
        <div class="resource-item">
          <span>模型</span>
          <a href="https://huggingface.co/collections/sensenova/sensenova-si" target="_blank" rel="noreferrer">Hugging Face 集合</a>
        </div>
      </div>
    </section>

    <section id="citation" class="chapter reveal" data-section>
      <h2>引用</h2>
      <pre id="citation-text" class="citation">@article{cai2025scaling,
  title={Scaling Spatial Intelligence with Multimodal Foundation Models},
  author={Cai, Zhongang and Wang, Ruisi and Gu, Chenyang and Pu, Fanyi and Xu, Junxiang and Wang, Yubo and Yin, Wanqi and Yang, Zhitao and Wei, Chen and Sun, Qingping and Zhou, Tongxi and Li, Jiaqi and Pang, Hui En and Qian, Oscar and Wei, Yukun and Lin, Zhiqian and Shi, Xuanke and Deng, Kewang and Han, Xiaoyang and Chen, Zukai and Fan, Xiangyu and Deng, Hanming and Lu, Lewei and Pan, Liang and Li, Bo and Liu, Ziwei and Wang, Quan and Lin, Dahua and Yang, Lei},
  journal={arXiv preprint arXiv:2511.13719},
  year={2025}
}</pre>
      <button class="copy-btn" data-copy-target="citation-text">复制 BibTeX</button>
    </section>

    <footer class="footer">WAP - standalone paper narratives for every reader.</footer>
  </div>

  <script src="/papers/https-arxiv-org-abs-2511-13719/script.js"></script>
</body>
</html>
